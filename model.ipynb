{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNKOMpWzBQflthmT3zbhv8X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["可参考的code：  \n","1.《Unsupervised Sentence Compression using Denoising Auto-Encoders》：https://github.com/zphang/usc_dae  \n","2.《Unsupervised Abstractive Sentence Summarization using Length Controlled Variational Autoencoder》：https://github.com/raphael-sch/SumVAE  \n","3.《SEQ^3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression》：https://github.com/cbaziotis/seq3  \n","4.《Get To The Point: Summarization with Pointer-Generator Networks》：  \n","Tensorflow：https://github.com/abisee/pointer-generator  \n","PyTorch：https://github.com/atulkum/pointer_summarizer  \n","https://github.com/atulkum/pointer_summarizer  \n","5.pointer network：  \n","Tensorflow：https://github.com/devsisters/pointer-network-tensorflow  \n","PyTorch：https://github.com/shirgur/PointerNet  \n","5.Transformer：https://github.com/huggingface/transformers  \n","http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n","https://github.com/tensorflow/tensor2tensor"],"metadata":{"id":"lG2cQ3OT5_OS"}},{"cell_type":"markdown","source":["**Modules**  \n","1.Embedding  \n","2.Network  \n","&emsp;2.1 Encoder1  \n","&emsp;2.2 Encoder2  \n","&emsp;2.3 Decoder  \n","&emsp;2.4 Discriminator  \n","&emsp;2.5 Loss calculation:  \n","&emsp;&emsp;2.5.1 loss1  \n","&emsp;&emsp;2.5.2 loss2  \n","&emsp;&emsp;2.5.3 loss3  \n","&emsp;&emsp;2.5.4 loss4  \n","3.Train"],"metadata":{"id":"PGywhrlL-uU6"}},{"cell_type":"markdown","source":["**Variants**  \n","1.Encoder1/Encoder2  \n","&emsp;1.1 RNN encoder  \n","&emsp;1.2 Transformer encoder  \n","&emsp;1.3 pointer network encoder  \n","2.Decoder  \n","3.Discriminator"],"metadata":{"id":"eUtUQZhNAZ-N"}},{"cell_type":"markdown","source":["**LIBS**"],"metadata":{"id":"zLJYDLaMPcGN"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F"],"metadata":{"id":"zXmgN6TCPT2B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**DATA & EMBEDDING**"],"metadata":{"id":"OAxUGc8yPrLI"}},{"cell_type":"code","source":["pass"],"metadata":{"id":"g04VmnEbPvFO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NETWORK**"],"metadata":{"id":"MqxawcmIHitO"}},{"cell_type":"code","source":["class EEDDModel(nn.Module):\n","  def __init__(Encoder1, Encoder2, Decoder, Discriminator, params):\n","    self.encoder1 = Encoder1(params['encoder1'])\n","    self.encoder2 = Encoder2(params['encoder2'])\n","    self.decoder = Decoder(params['decoder'])\n","    self.discriminator = Discriminator(params['discriminator'])\n","  \n","  def forward(self, embedded_inputs):\n","    abstract = self.encoder1(embedded_inputs)\n","    minor_info = self.encoder2(embedded_inputs)\n","    full_info = torch.cat(abstract, minor_info, dim = -1)\n","    recon1 = self.decoder(abstract)\n","    recon2 = self.decoder(minor_info)\n","    recon3 = self.decoder(full_info)\n","    return abstract, recon1, recon2, recon3\n"],"metadata":{"id":"xu3OMsnG59ql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**LOSS**"],"metadata":{"id":"JWkAN0giHmtV"}},{"cell_type":"code","source":["class EEDDLoss(nn.Module):\n","  def __init__(loss_ratios):\n","    self.ratios = loss_ratios\n","  \n","  def forward(self, origin, abstract, recon1, recon2, recon3, real_sentence):\n","    l1 = ReconLoss(origin, recon1)\n","    l2 = ReconLoss(origin, recon2)\n","    l3 = ReconLoss(origin, recon3)\n","    l4 = DiscriminatorLoss(real_sentence, abstract)\n","    loss = ratios[0]*l1 - ratios[1]*l2 + ratios[2]*l3 + ratios[3]*l4\n","    return loss"],"metadata":{"id":"HvSswhV3CfQ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TRAIN**"],"metadata":{"id":"P2losJu2Hslf"}},{"cell_type":"code","source":["def train(loss_ratios, model, Optimizer, opt_params, epochs, trainLoader, device):\n","  criterion = EEDDLoss(loss_ratios)\n","  optimizer = Optimizer(model.parameters(), lr=opt_params['lr'], momentum=opt_params['momentum'])   # eg: Optimizer = torch.optim.SGD\n","  model.to(device)\n","  for e in range(epochs):\n","    for i, embedded_inputs in enumerate(trainLoader):\n","      embedded_inputs = embedded_inputs.to(device)\n","      abstract, recon1, recon2, recon3 = model(embedded_inputs)\n","      loss = criterion(embedded_inputs, abstract, recon1, recon2, recon3, embedded_inputs)    # real_sentence is set as the original sentence for unsupervise\n","      optimizer.zero_grad()   # if don't call zero_grad, the grad of each batch will be accumulated\n","      loss.backward()\n","      optimizer.step()\n","      if i % 20 == 0:\n","        print('epoch: {}, batch: {}, loss: {}'.format(e+1, i+1, loss.data))\n","  torch.save(model, 'EEDD.pth')"],"metadata":{"id":"0LnwNKkDMYFl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# back up\n","\n","from modules.training.base_trainer import BaseTrainer\n","\n","class EEDDTrainer(BaseTrainer):\n","  def __init__(model, train_loader, valid_loader, criterion, optimizers, config, device, batch_end_callbacks=None, loss_ratios):\n","    super().__init__(train_loader, valid_loader, config, device, batch_end_callbacks)\n","    self.model = model\n","    self.criterion = criterion\n","    self.optimizers = optimizers\n","  \n","  def train_epoch(self):\n"],"metadata":{"id":"6LGrJmpb7_aP"},"execution_count":null,"outputs":[]}]}